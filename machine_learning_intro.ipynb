{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure pandas and sklearn are installed!\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titanic problem. In this problem we are given data on passengers on the titanic. \n",
    "\n",
    "Our goal: build a predictive model that can accurately predict who will survive the titanic tragedy.\n",
    "\n",
    "First, letâ€™s create DataFrames of our data. Our data is split into two parts, first is the training data, which we will use to build our model. The second is our test data, this is used to test our final results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/titanic_train.csv\")\n",
    "test = pd.read_csv(\"data/titanic_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Take a quick peak into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Meaning of Variables\n",
    "from IPython.display import Image\n",
    "Image(\"data/data_dictionary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We will drop complicated features and convert string objects to numeric via one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of train data frame before preprocessing: {train.shape}\")\n",
    "\n",
    "train.drop([\"Name\",\"Ticket\",\"Cabin\"],axis=1,inplace=True) # Drop Name and Tickets\n",
    "test.drop([\"Name\",\"Ticket\",\"Cabin\"],axis=1,inplace=True) # Drop Name and Tickets\n",
    "\n",
    "# Impute missing values as the mean value of column in training data\n",
    "train.fillna(train.mean(),inplace=True) \n",
    "test.fillna(train.mean(),inplace=True) \n",
    "\n",
    "train = pd.get_dummies(train,dummy_na=True) # One Hot Encode Features\n",
    "test = pd.get_dummies(test,dummy_na=True) # One Hot Encode Features\n",
    "\n",
    "print(f\"Shape of train data frame after preprocessing: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will split our training data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target \n",
    "target = train[\"Survived\"]\n",
    "\n",
    "# Drop Target from train set\n",
    "del train[\"Survived\"]\n",
    "\n",
    "# Make Splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train.drop([\"PassengerId\"],axis=1),target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split up our data, let's build a random forest and adaboost classifiers for this data.\n",
    "\n",
    "We will fit the models using train, and validate using validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest \n",
    "rf = RandomForestClassifier(n_estimators = 500) # Create Random Forest Object\n",
    "rf.fit(X_train,y_train) # Fit Random Forest object\n",
    "y_hat_rf = rf.predict(X_valid) # Predict on Valdiation Set\n",
    "print(f\"The accuracy of random forest is {rf.score(X_valid,y_valid)}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit AdaBoost\n",
    "adb = AdaBoostClassifier(n_estimators=30) # Create Adaboost Object\n",
    "adb.fit(X_train,y_train) # Fit Adaboost object\n",
    "y_hat_adb = rf.predict(X_valid) # Predict on Valdiation Set\n",
    "print(f\"The accuracy of adaboost is {adb.score(X_valid,y_valid)}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, these models have done a pretty good job at predicting whether or not somebody survived! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance let's us look under the hood at our model.\n",
    "\n",
    "We compare the what features each model though were important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(obj,columns=X_train.columns):\n",
    "    model_type = str(obj.__class__)[:-2].split(\".\")[-1] # Get Model Type Name\n",
    "    pd.Series(obj.feature_importances_,index=columns).sort_values(ascending=True).plot(kind=\"barh\",title=model_type+\" Importance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_importance(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission\n",
    "\n",
    "If we choose to submit the Random Forest model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(test.drop([\"PassengerId\"],axis=1)) \n",
    "sub = pd.DataFrame({\"PassengerId\":test[\"PassengerId\"],\"Survived\":pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submissions/rf_sub.csv\",index=False) # LB .0.74641"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My submission performed 0.74641 on the leader board. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_science_club]",
   "language": "python",
   "name": "conda-env-data_science_club-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
